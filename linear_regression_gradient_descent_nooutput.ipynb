{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Efficiency Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and objectives \n",
    "\n",
    "This dataset is obtained from UCI machine learning repository\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Energy+efficiency \n",
    "\n",
    "There are serveral other online repositories to get datasets for machine learning.\n",
    "Check for example, https://machinelearningmastery.com/a-guide-to-getting-datasets-for-machine-learning-in-python/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to predict the heating and cooling load requirements (i.e. energy efficiency) given the building characteristics.\n",
    "\n",
    "In the dataset, there are 768 samples describing different building shapes with eight  characteristics:\n",
    "\n",
    "    X1 Relative Compactness\n",
    "    X2 Surface Area\n",
    "    X3 Wall Area\n",
    "    X4 Roof Area\n",
    "    X5 Overall Height\n",
    "    X6 Orientation\n",
    "    X7 Glazing Area\n",
    "    X8 Glazing Area Distribution\n",
    "    \n",
    "The corresponding outputs/labels are given as: \n",
    "    y1 Heating Load\n",
    "    y2 Cooling Load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Elementary question               \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import necessary python libraries and setup the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Routines for linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Required for splitting the dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set label size for plots\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code segment reads the ENB2012 dataset as a pandas data frame using pandas.read_csv \n",
    "More information with examples can be found at https://machinelearningmastery.com/massaging-data-using-pandas/ \n",
    "\n",
    "Also keep in mind that there are several other ways to read data in Python, e.g. using numpy.loadtext, numpy.read_csv, numpy.loadtxt, numpy.genfromtxt(), pandas.read_csv(), etc. Read more at https://machinelearningmastery.com/load-machine-learning-data-python/ \n",
    "\n",
    "Note: splitting is done randomly and hence every time you ran the program, you obtain different results.\n",
    "To ensure that you always get the same results, set the seed of the random generator. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'Y1', 'Y2']\n",
    "df = pd.read_csv('ENB2012-data.csv', names = columns, skiprows = 1)\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore your training dataset \n",
    "\n",
    "## 3.1 Descriptive statistics \n",
    "\n",
    "Get some statistical summary of the dataset using `pandas.describe` and the correlation between each pair of variables using `pandas.corr`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistical summary of the train dataset\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "print(train.describe()\n",
    "     )\n",
    "\n",
    "\n",
    "# END of YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print correlation of variables using the train dataset \n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "print(train.corr())\n",
    "     \n",
    "\n",
    "# END of YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Scatter diagrams\n",
    "It is important to explore the dataset before conducting any machine learning work. For example, draw scatter diagrams for each predictor vs each target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter diagrams for each predictor VS each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=len(train.columns)\n",
    "fig,axis=plt.subplots(length-2,2,figsize=(15,15))\n",
    "for i in range(0,2):\n",
    "    for j in range(0,length-2):\n",
    "        x_axis=train[train.columns[j]]\n",
    "        y_axis=train[train.columns[i-2]]\n",
    "        ax= axis[j][i].scatter(x_axis,y_axis)\n",
    "        axis[j][i].set_title(\"Y\" +str(i+1)+ \"VS X\" + str(j+1) )\n",
    "    \n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict target without using predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean of Y1 and Y2 to predict each target without knowledge of predictors. \n",
    "In this case, the mean squared error (MSE) associated with the prediction is simply the variance of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and variance for each target variable \n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "\n",
    "print( \n",
    "    'Prediction of Y1: %.4f'\n",
    "      % train[\"Y1\"].mean() ,\n",
    "      '\\nMSE: %.4f' %  \n",
    "      train[\"Y1\"].var() , \n",
    "      '\\nPrediction of Y2: %.4f'%\n",
    "      train[\"Y2\"].mean(), \n",
    "      \"\\nMSE: %.4f\"% \n",
    "      train[\"Y2\"].var() \n",
    "\n",
    "     )\n",
    "# END of YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predict target using a single predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit a linear regression model,  use `sklearn.linear_model.LinearRegression()` and complete the code snippet below to define a function, `one_feature_regression` that takes `x_train`, `y_train`, `x_test` and `y_test` and fits a linear regressor to predict y. It then plots the data along with the resulting line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_feature_regressor(x_train, y_train, x_test, y_test):  \n",
    "    ### Your code starts here ###\n",
    "    x_train = np.array(x_train).reshape(-1,1)\n",
    "    x_test = np.array(x_test).reshape(-1,1)\n",
    "    y_train = np.array(y_train).reshape(-1,1)\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    \n",
    "\n",
    "    #Craete an object of linear regression and fit a model\n",
    "    regr = linear_model.LinearRegression()\n",
    "    \n",
    "    regr.fit(x_train,y_train)\n",
    "\n",
    "    \n",
    "    # Make predictions using the model \n",
    "    y_pred = regr.predict(x_test)\n",
    "\n",
    "    ### End of your code  ###\n",
    "\n",
    "    # Plot test data points as well as predictions\n",
    "    plt.scatter(x_test, y_test)\n",
    "    plt.scatter(x_test, y_pred)\n",
    "    plt.xlabel('X', fontsize=14)\n",
    "    plt.ylabel('Y', fontsize=14)\n",
    "    plt.show()\n",
    "    print (\"MSE: %.5f\" %  mean_squared_error(y_test, y_pred))\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testin the defined `one_feature_regression` function with feature X2 to predict Y1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr= one_feature_regressor(train['X2'],train['Y1'],test['X2'],test['Y1'])\n",
    "print (\"w = %.5f\" % regr.coef_[0][0])\n",
    "print (\"b = %.5f\" % regr.intercept_[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predict target using a subset of features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict a target using more than one feature. Complete the code for the following function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_feature_regressor(x_train, y_train):\n",
    "    # YOUR CODE GOES HERE\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    #fitting the data to the linear regression model\n",
    "    regr.fit(x_train,y_train)\n",
    "    \n",
    "    \n",
    "    # Make predictions using the model \n",
    "    y_pred = regr.predict(x_train)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # END of YOUR CODE\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = subset_feature_regressor(train[['X1', 'X2']], train['Y1'])\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"MSE: \", mean_squared_error(test['Y1'], regr.predict(test[['X1', 'X2']])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use all 8 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = subset_feature_regressor(train.iloc[:,0:8], train['Y1'])\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"MSE: \", mean_squared_error(test['Y1'], regr.predict(test.iloc[:,0:8])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2a: Implement an iterative solution  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you you are required to implement the iterative (gradient descent) solution. The method should take features `x` and predictions `y` of the training set and return back the parameter values including the bias term. You should also initialize the hyper-parameters in the beginning of the method. Also, plot the the cost function at different iterations.\n",
    "Here, the input consists of:\n",
    "* training data `x`, and  `y`, which are numpy arrays of dimension `m`-by-`n` and `m`, respectively (if there are `m` training points and `n` features)\n",
    "\n",
    "The function should find the `n`-dimensional vector `w` and offset `b` that minimize the MSE loss function, and return:\n",
    "* `w` and `b`\n",
    "* `losses`, an array containing the MSE loss at each iteration\n",
    "\n",
    "<span style=\"color:red\">Note:</span> First read and undertand the lecture material. Next, when implementing gradient descent, think carefully about two issues.\n",
    "\n",
    "1. What is the step size (learning rate)?\n",
    "2. When has the procedure converged?\n",
    "\n",
    "Take the time to experiment with different ways of handling these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_GD(X, Y, iteration = 10000, learning_rate = 1e-7,threshhold = 1e-7):\n",
    "    # inputs: trainx and trainy, the features and the target in the training set\n",
    "    # output: a vector of weights including the bias term\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "     #setting up the parameters\n",
    "    \n",
    "    X=np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    cost = []\n",
    "    error = []\n",
    "    omega = []\n",
    "    bias = []\n",
    "    iterations = []\n",
    "    W = np.full((X.shape[1], 1), np.random.randint(-1,1))\n",
    "    b = np.random.randint(0,80)\n",
    "    n =  float(len(X))\n",
    "    prev_cost = 0.0\n",
    "    \n",
    "\n",
    "    \n",
    "    #The iterative loop to update the parameters\n",
    "    for i in range(iteration):\n",
    "        \n",
    "\n",
    "        #The linear functions and the gradients\n",
    "        Y_hat = np.dot(X , W) + b\n",
    "        gradw =  (np.dot(-(X.T),(Y - Y_hat))) / n \n",
    "        gradb = float(np.mean( Y - Y_hat ))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #The update value of W vector and bias term \n",
    "        W = W - (gradw * learning_rate)\n",
    "        b = b - (gradb * learning_rate)\n",
    "        \n",
    "        \n",
    "        current_cost = float(mean_squared_error(Y,Y_hat))\n",
    "        current_error = float(np.mean((Y-Y_hat)**2))\n",
    "        current_W = W\n",
    "        current_b = b\n",
    "        current_i = i\n",
    "        \n",
    "        cost.append(current_cost)\n",
    "        error.append(current_error)\n",
    "        omega.append(current_W)\n",
    "        bias.append(current_b)\n",
    "        iterations.append(i)\n",
    "        \n",
    "        \n",
    "        if abs(prev_cost-current_cost)<=  threshhold:\n",
    "            break\n",
    "    \n",
    "    \n",
    "        prev_cost = current_cost\n",
    "                # Printing the parameters for each 1000th iteration\n",
    "        print(f\"Iteration {i+1}: Cost {current_cost}, Weight \\n {W}, \\n Bias {b} \")\n",
    "    \n",
    "    #plot the cost for each iteration\n",
    "    plt.scatter(iterations, cost)\n",
    "    return W , b , current_cost\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # END of YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_GD(train.iloc[:,0:8], train[['Y1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2b: Using your iterative approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit a modetl to predict Y1 using X2 based on the training training data in Question 1 \n",
    "* Predict Y1 using the testing data and compare the results with those obtained in Question 1\n",
    "* Write your comments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the multi-feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "w , b , cost_multi =linear_regression_GD(train[['X2']],train[['Y1']])\n",
    "\n",
    "# END of YOUR CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = np.dot(test.[['X2']],w) + b\n",
    "YY = test.X2 * float(w) + b\n",
    "\n",
    "\n",
    "# Plotting the regression line\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(test.X2, test.Y1, marker='o', color='red')\n",
    "#plt.plot(test.X2, Y_predd, color='blue',markerfacecolor='red', markersize=10,linestyle='dashed')\n",
    "plt.scatter(test.X2 , Y)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estimated Weight: {float(w)}\\nEstimated Bias: {float(b)} \\nCost: {mean_squared_error(YY, test.Y1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see that in the method we did which is the iterative approach, we get MSE = 63 if we set the learning rate at the right amount. which is lower than sklearn function, the number of steps \"iterations\" should be at an amount where the function converges \"cost[i] - cost [i-1] = $\\epsilon$, and the learning rate of the step size depends on the sample itself, we can set the learning rate to be propotional to the second derivative of the loss function $$ \\frac{\\partial^2}{\\partial \\theta_j^2} J(\\theta_0,\\theta_j)=\\frac{1}{m}\\sum_i^m x_i^2$$ \n",
    " But in here I took it as a small scalar because I didn't get consistent results with having the learning rate = second partial derivative of the w or $\\theta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  Appendix: Another method by using one feature only  <span style=\"color:red\">\n",
    "by using this method we can see the relation between the weight vector and the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_one_feature(x, y, iterations=5000, learning_rate = 1e-3,\n",
    "                     stopping_threshold = 1e-8):\n",
    "     \n",
    "    # Initializing weight, bias, learning rate and iterations\n",
    "    w = np.random.randint(-1,0)\n",
    "    b = np.random.randint(-100,100)\n",
    "    iterations\n",
    "    learning_rate\n",
    "    n = float(len(x))\n",
    "     \n",
    "    costs = []\n",
    "    weights = []\n",
    "    prev_cost = 0.0\n",
    "     \n",
    "    # Estimation of optimal parameters\n",
    "    for i in range(iterations):\n",
    "         \n",
    "        # Making predictions\n",
    "        y_hat = (w * x) + b\n",
    "         \n",
    "        # Calculationg the current cost\n",
    "        current_cost = mean_squared_error(y, y_hat)\n",
    " \n",
    "        # If the change in cost is less than or equal to\n",
    "        # stopping_threshold we stop the gradient descent\n",
    "        if prev_cost and abs(prev_cost-current_cost)<=stopping_threshold:\n",
    "            break\n",
    "         \n",
    "        prev_cost = current_cost\n",
    " \n",
    "        costs.append(current_cost)\n",
    "        weights.append(w)\n",
    "         \n",
    "        # Calculating the gradients\n",
    "        dldw = -(1/n) * np.sum(x * (y-y_hat))\n",
    "        dldb = -(1/n) * np.sum(y-y_hat)\n",
    "         \n",
    "        # Updating weights and bias\n",
    "        w = w - (learning_rate * dldw)\n",
    "        b = b - (learning_rate * dldb)\n",
    "                 \n",
    "        # Printing the parameters for each 1000th iteration\n",
    "        print(f\"Iteration {i+1}: Cost {current_cost}, Weight \\\n",
    "        {w}, Bias {b}\")\n",
    "     \n",
    "     \n",
    "    # Visualizing the weights and cost at for all iterations\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.plot(weights, costs)\n",
    "    plt.scatter(weights, costs, marker='o', color='red')\n",
    "    plt.title(\"Cost vs Weights\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.xlabel(\"Weight\")\n",
    "    plt.show()\n",
    "     \n",
    "    return w, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_weight, eatimated_bias, cost = gradient_descent_one_feature(train['X2'], train.Y1, iterations=10000, learning_rate=1e-7)\n",
    "print(f\"Estimated Weight: {estimated_weight}\\nEstimated Bias: {eatimated_bias} \\nCost: {cost[-1]}\")\n",
    " \n",
    "    \n",
    "ys = estimated_weight*test.X2 + eatimated_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the regression line\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(test.X2, test.Y1, marker='o', color='red')\n",
    "#plt.plot(test.X2, Y_predd, color='blue',markerfacecolor='red', markersize=10,linestyle='dashed')\n",
    "plt.scatter(test.X2 , ys)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estimated Weight: {estimated_weight}\\nEstimated Bias: {eatimated_bias} \\nCost: {mean_squared_error(ys,test.Y1)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f034faa9436916d2fe26a4184004d6a846ac49d8ff62943d73e298b6be9969c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
